{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y4okRUhPjC9"
   },
   "source": [
    "# Final Project Baseline: House Prices- Advanced Regression Techniques\n",
    "### Author: Radia Abdul Wahab, Parham Motameni, Jun Qian\n",
    "### Date: Fall 2021\n",
    "### Course: w207 Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN7Ygmt9PjDB"
   },
   "source": [
    "## **Table of Contents:**\n",
    "\n",
    "#### **Project Summary**\n",
    "\n",
    "#### **Description of Data and Data source**\n",
    "\n",
    "#### **Installations and Libraries**\n",
    "\n",
    "#### **Dataset Exploratory Analysis**\n",
    "\n",
    ">Nominal/Ordinal data\n",
    "\n",
    ">Categorical data\n",
    "\n",
    "\n",
    "#### **Step-By-Step performance of algorithms**\n",
    "\n",
    ">OLS regression\n",
    "\n",
    ">Decision tree\n",
    "\n",
    ">Random forest\n",
    "\n",
    "\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "#### **Further work for final submission**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AP32YpP3b9FE"
   },
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJE3XQpbPjDC"
   },
   "source": [
    "## **Motivation**:\n",
    "How much would you pay for a house? That is often one of the hardest questions to answer. When buying a house or selling a house, it is very crucial to determine the right pricing, since house prices change over time and each house its own \"$ rating\". \n",
    "\n",
    "Three main aspects determine the price of a house. \n",
    "1. Condition\n",
    "2. Features (Number of rooms, square footage etc)\n",
    "3. Location\n",
    "    \n",
    "These 3 properties can be defined under a large set of sub-properties. The **Ames Housing dataset** was compiled by **Dean De Cock**, with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. This data set (https://www.kaggle.com/c/house-prices-advanced-regression-techniques) is being used for this project to demonstrate the use of various Machine Learning techniques, to be able to have the algorithm perform the difficult task of deciding what the house price should be. \n",
    "\n",
    " \n",
    "## **Baseline Project Goals**: Through this baseline project we are trying to answer 3 main questions:\n",
    "1. What combination of aspects of a house most determines the sale-price?\n",
    "2. Can Regression based prediction be used to estimate the price of the house?\n",
    "3. What is the best accuracy that can be achieved by using such algorithms?\n",
    "4. Are there any other ML models which can achieve a better prediction?\n",
    "\n",
    "\n",
    "## **Baseline Overall Strategy**: The general strategy we followed to come to the conclusion has been:\n",
    "1. Thorough review of the description of data provided (including background research)\n",
    "2. Reviewing ALL of the variables, in order to avoid omitted variable bias. \n",
    "3. EDA of ALL integer based paremeters\n",
    "4. EDA ordinal/categorical parameters  \n",
    "5. Shortlisting parameters that show strong trends but control potential collearity\n",
    "6. Perform Regression using the shortlisted parameters\n",
    "7. First pass accuracy assessment of basic linear regression without any data cleaning\n",
    "to be used in KNN/decision tree based algorithms for later stages of project including final submission\n",
    "\n",
    "\n",
    "\n",
    "We have answered the above questions stated under \"Baseline Project Goals\" by Exploring the data (EDA) and performing Linear Regression on a selected set of parameters. The outcome is delailed below\n",
    "\n",
    "## **Outcome**:\n",
    "\n",
    "1. The 10 important parameters are: ...\n",
    "2. Linear regression line fit can be obtained\n",
    "3. Accuracy of prediction with a linear regression fit is >80%\n",
    "\n",
    "## **Baseline Submission Content**:\n",
    "For this baseline submission we show an overview of exploratory data analysis (EDA), and demonstrate the data  with sufficient visuals. We then set up a pipeline to demonstrate feasibility of using this data to perform prediction on house prices. \n",
    "\n",
    "In this report we have also included:\n",
    "\n",
    "1. The format of the data\n",
    "2. The various paremeters given\n",
    "3. Correlation and Distribution of the integer based parameters by visualization\n",
    "4. Boxplot of the categorical parameters \n",
    "5. Short listed parameters and justification for choice\n",
    "6. Demonstration of a simple regression algorithm to show a regression fit\n",
    "7. Visualizaion of regression line fit \n",
    "8. Visualization of actual vs predicted values to demonstrate fit\n",
    "9. Accuracy estimates\n",
    "10. Summary Statistics?\n",
    "11. Conclusion\n",
    "12. Further work\n",
    "   \n",
    "\n",
    "## **Further work**:\n",
    "\n",
    "1. Apply alternative feature selection algorithms \n",
    "2. Descripencies of those parameters and how those will effect us\n",
    "3. Look further into regression results interpretation. Potentially also advanced regression techniques(tbd)\n",
    "3. Removing outliers, redoing linear regression, recalculating accuracy to show improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3zbP1UlPjDD"
   },
   "source": [
    "# Description of Data and Data Source\n",
    "\n",
    "- **Data Source**:\n",
    "\n",
    "- **Description of Data**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z-1yFvJPjDF"
   },
   "source": [
    "## Import all Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ncAF-sIOPjDG"
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# sklearn\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "# internal modules\n",
    "from elastic_net_regressor import ElasticNetRegressor\n",
    "from linear_regressor import LinearRegressor\n",
    "from decision_tree_regressor import DecisionTreeRegressor\n",
    "from lasso_regressor import LassoRegressor\n",
    "from random_forest_regressor import RandomForestRegressor\n",
    "from ridge_regressor import RidgeRegressor\n",
    "from neuralnetwork import create_nn_regressor\n",
    "from data_loader import DataLoader\n",
    "from configurations import args\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjVSZGLoPjDH",
    "outputId": "5e735da7-ff4e-47c6-a16c-2f752100c2dd"
   },
   "outputs": [],
   "source": [
    "# download data file if it is not ready\n",
    "data_file = Path(args.data_path)\n",
    "if data_file.is_file():\n",
    "    print(\"Datafile is already loaded.\")\n",
    "else:\n",
    "    !curl -L \"https://drive.google.com/uc?export=download&id=1ortEJfmlpt9-dbg5f6cTDt5nicswf5wT\" > 'test.csv'\n",
    "    !curl -L \"https://drive.google.com/uc?export=download&id=1EG5jP5RDEIsNAMaF2m42KOyz-ZVjC8wS\" > 'train.csv'\n",
    "data_loader = DataLoader(args)\n",
    "df = data_loader.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRNfelLOPjDH"
   },
   "outputs": [],
   "source": [
    "# lock the seed to have repeatable results\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXkZWr15PjDJ"
   },
   "source": [
    "## Taking a look at what the features are\n",
    "Additionally reviewed the data_description.txt file. The column names and the names on the file correspond accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbpmtpSeb9FK",
    "outputId": "0fe6df6e-c9e0-4a18-8212-f4076f83a739"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qdv_YbEPjDK"
   },
   "source": [
    "## Check whether there are variables with significant missing values. \n",
    "\n",
    "It looks like PoolQC , MiscFeature, Alley, Fence, FireplaceQu,LotFrontage have substantial missing values. \n",
    "\n",
    "Looking closer into the descriptions of these variables, we find most NA actually means the house does not have such feature. So we fill it with \"NonExist\" as a distinct category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QlzcjcwFPjDL",
    "outputId": "a9f4ac59-0e15-4a8a-b5c5-f41007af8721"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iQq1NdRPjDL",
    "outputId": "231f4b74-cd6c-4d62-dea5-3ccf757c7265"
   },
   "outputs": [],
   "source": [
    "# Fill missing value of 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu' as NonExist. Leave LotFrontage for further investigation\n",
    "df[['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']]=df[['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']].fillna('NonExist')\n",
    "df.isnull().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7I5yZrSPjDI"
   },
   "source": [
    "## Split training data into our own train and test data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhJcAh0wb9FM"
   },
   "source": [
    "#### The \"test.csv\" is an unlabelled set. Therefore in order to assess performance we are splitting the train.csv into our own training, development and test set as needed\n",
    "#### We are using split() to ensure random distribution of data points\n",
    "#### We are starting off with ~10% of the train set into a test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1JUDtInxPjDI"
   },
   "outputs": [],
   "source": [
    "# Create train, dev, test datasets\n",
    "\n",
    "# split data into Test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_loader.df_X, data_loader.df_y, test_size=0.10, random_state=1)\n",
    "\n",
    "# TODO split more to dev set if needed!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkp5NNh9PjDI"
   },
   "source": [
    "## Initial Look at the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "pnj0xm0RPjDI",
    "outputId": "7e67a9cb-e686-4e92-dd3f-462c191c0c32"
   },
   "outputs": [],
   "source": [
    "print(f'X_train size: {X_train.shape}, X_test size: {X_test.shape}')\n",
    "print(f'yy_train size: {y_train.shape}, y_test size: {y_test.shape}')\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH1e3PE7b9FN"
   },
   "source": [
    "Above result shows that\n",
    "1.  The **train** set has **1314** records (1314 labelled houses)\n",
    "2.  The **test** set has **146 long** records (146 labelled houses)\n",
    "3.  Each house has **79 features**. \n",
    "4.  Features are a mix of **nominal, ordinal, and categorical**\n",
    "5.  We will have to be cognisant of the different data types for our assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtwWdMh_b9FN"
   },
   "source": [
    "\n",
    "### Analyzing bathroom features impact on house price\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN2IAAq0b9FN"
   },
   "source": [
    "Here we analyze the relation between the number of bathrooms and the sales price. There are four features in the dataset presenting the bathrooms, full and half bathrooms for both basements and above the ground. Here we analyze each of the features individually and then in combined form. To combine them, we verify two approaches, one considering each half-bath as .5 of a full-bath, and for the other one, we consider each bath as one no matter if it is full or half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymJRlWz2b9FN"
   },
   "outputs": [],
   "source": [
    "def plot_features(feature_set, output_set, feature_per_row=2,\n",
    "    width=15, height=20):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    number_features = feature_set.shape[1]\n",
    "    num_rows = number_features // feature_per_row\n",
    "    num_rows = num_rows if number_features % feature_per_row == 0 \\\n",
    "         else num_rows + 1\n",
    "    k = 1\n",
    "    for feature in feature_set:\n",
    "        output = ['SalePrice']\n",
    "        plt.subplot(num_rows, feature_per_row, k)\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel('Sale Price')\n",
    "        plt.scatter(feature_set[feature].values, output_set[output].values)\n",
    "        k = k+1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "id": "TIrouZ3ub9FO",
    "outputId": "25a61208-fa5a-4db3-b465-0020edccb891"
   },
   "outputs": [],
   "source": [
    "# bathrooms impact on price\n",
    "def analyze_num_bath_impact():\n",
    "    f_baths = X_train[['BsmtFullBath',\n",
    "                            'BsmtHalfBath', 'FullBath', 'HalfBath']].copy()\n",
    "    plot_features(f_baths, y_train, feature_per_row=4, height=3)\n",
    "    f_baths['total_bath_half_as_full'] = \\\n",
    "        data_loader.get_bath_features_dataset(f_baths, consider_half_as_full=True)['total_bath']\n",
    "    plot_features(f_baths[['total_bath_half_as_full']],\n",
    "                  y_train, feature_per_row=1, height=3)\n",
    "    f_baths['total_bath_half_as_half'] = \\\n",
    "        data_loader.get_bath_features_dataset(f_baths, consider_half_as_full=False)['total_bath']\n",
    "    plot_features(f_baths[['total_bath_half_as_half']],\n",
    "                  y_train, feature_per_row=1, height=3)\n",
    "\n",
    "analyze_num_bath_impact()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qya6t65kb9FO"
   },
   "source": [
    "**Number of the bathrooms analysis result**\n",
    "\n",
    "The number of bathroom analysis shows that using the total number of bathrooms considering half-bath as half reveals more granular details about the relation between the number of bathrooms with the sales price better than other approaches. So we will use the half_as_half in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0tDIDkGPjDJ"
   },
   "source": [
    "## Analysis on SalePrice\n",
    "\n",
    "For SalePrice,  we use the mean price and variation range to determine whether we should seperate the sales price into before and after financial crisis. The observations are:\n",
    "1. We do see a trend change from upward to decrease/flat in price \n",
    "\n",
    "\n",
    "2. However, the difference is only around 5% with a similar variation. As a result, in this stage, we keep it as is for cross sectional analysis. We might look into it further later.\n",
    "\n",
    "\n",
    "3. We identify some months the sales price is significantly higher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "rBVPNHymPjDK",
    "outputId": "d818cfc6-fed7-4dc8-c17c-b3afa35d4c97"
   },
   "outputs": [],
   "source": [
    "#try to explore the impact of financial crisis to house prices#\n",
    "df_train=y_train.merge(X_train, how=\"inner\",left_index=True,right_index=True)\n",
    "df_train[['SalePrice','YrSold']].groupby(['YrSold']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "XWWJCIF1PjDK",
    "outputId": "9b26633b-2993-4687-9218-36815ac7bd6c"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x= df_train['YrSold'], y= df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "A5i97X0bb9FP",
    "outputId": "94a5ff52-d617-4f7a-b325-70ac2c15fd83"
   },
   "outputs": [],
   "source": [
    "df_train[['SalePrice','MoSold']].groupby(['MoSold']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "FSvgnqwHb9FP",
    "outputId": "86e1ea03-2766-41eb-d37c-a4dae8c25c3f"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x= df_train['MoSold'], y= df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SX-xo46ePjDJ"
   },
   "source": [
    "# First pass with nominal values\n",
    "\n",
    "First we work on the nominal variables.\n",
    "\n",
    "I would like to see how the data points are correlated and distributed, before any data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OJ465FvyPjDL"
   },
   "outputs": [],
   "source": [
    "#pass training dataset to a sepearate EDA dataframe\n",
    "df_EDA=y_train.merge(X_train, how=\"inner\",left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA026XoVPjDL"
   },
   "source": [
    "## First step exploratory data analysis:\n",
    "\n",
    "We classify the features into two types of variables: Numerical and Categorical. \n",
    "\n",
    "For all the numerical variables, we plot the correlations heatmap between the dependent variable SalePrice and all the features. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the shortlisted feature list, run a for loop to plot all the data points\n",
    "# int_feature_list has been created a few cells above\n",
    "# int_feature_list contains all the columns that have some sort of numbers in them (int, float etc)\n",
    "def plot_numerical_scatter_plots():\n",
    "\n",
    "    ## all numeric variables##\n",
    "    # TODO: Not all non object ones are Numerical, for example the Overal Cond\n",
    "    df_num_list = df_EDA.select_dtypes(exclude={'object'}).columns.to_list()\n",
    "    df_num_list.remove('SalePrice')\n",
    "    g = sns.FacetGrid(pd.DataFrame(df_num_list), col=0, col_wrap=3, aspect=1.3,\n",
    "    sharex=False)\n",
    "    for ax, x_var in zip(g.axes, df_num_list):\n",
    "        sns.scatterplot(data=df_EDA, x=x_var, y='SalePrice', ax=ax)\n",
    "    g.tight_layout()\n",
    "    \n",
    "plot_numerical_scatter_plots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y4okRUhPjC9"
   },
   "source": [
    "# Final Project Baseline: House Prices- Advanced Regression Techniques\n",
    "### Author: Radia Abdul Wahab, Parham Motameni, Jun Qian\n",
    "### Date: Fall 2021\n",
    "### Course: w207 Machine Learning\n",
    "\n",
    "\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4vd7o6eb9FQ"
   },
   "source": [
    " ## Now we look at the correlation between these features as well as with the SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_core_matrix():\n",
    "    df_num_list = df_EDA.select_dtypes(exclude={'object'}).columns.to_list()\n",
    "    return df[df_num_list].corr()\n",
    "\n",
    "\n",
    "corr_matrix = get_core_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "X_ihM45YPjDL",
    "outputId": "37feca34-37dd-451a-ff8a-9534ae5cdcee"
   },
   "outputs": [],
   "source": [
    "def plot_heatmap_for_all_features(corr_matrix):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix)\n",
    "    plt.title(\"Correlation heatmap\")\n",
    "\n",
    "plot_heatmap_for_all_features(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "NGlPRfs1PjDM",
    "outputId": "b9473e9c-0743-4854-e519-87a72010aa3f"
   },
   "outputs": [],
   "source": [
    "##Pick the top 15 correlated features##\n",
    "def plot_heatmap_for_top_15(corr_matrix):\n",
    "    top15_corr = corr_matrix.sort_values('SalePrice', ascending= False)[0:15]\n",
    "    top15_corr = top15_corr.loc[:, top15_corr.index]\n",
    "\n",
    "    fig = plt.figure(figsize= (8, 8))\n",
    "    sns.heatmap(top15_corr, annot= True)\n",
    "    plt.title(\"Top 15 heatmap\")\n",
    "\n",
    "\n",
    "plot_heatmap_for_top_15(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjnhYj-ePjDR"
   },
   "source": [
    "### In this section we review ALL the variables with the plots above as well as the description provided to us in data_description.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **MSSubClass**: Identifies the type of dwelling involved in the sale.\n",
    "\n",
    "Need to convert this to categorical, or ordinal. For Baseline we will not be using this. In terms of conversion to ordinal, this is not ordered currently. We will need to identify what criteria should be used to order these. However, this variable determines the type of property and is ultimately a critical feature that needs to be captured in the final ML algorithm.\n",
    "\n",
    "2. **LotFrontage**: Linear feet of street connected to property\n",
    "\n",
    "Need to remove values above 200. They are clearly outliers that are skewing the data on the right\n",
    "\n",
    "Once we remove the outliers, we should see a good correlation, visually\n",
    "\n",
    "3. **LotArea**: Lot size in square feet\n",
    "\n",
    "This is an important parameter\n",
    "\n",
    "However, looking at the scatter plot, I would say there are a few outliers that significantly skew the data\n",
    "\n",
    "we should remove any values that are >50K and perform our final analysis.\n",
    "\n",
    "Once we remove the outliers above 50k, we should get a reasonable correlation\n",
    "\n",
    "4. **OverallQual**: Rates the overall material and finish of the house\n",
    "\n",
    "Similar to the MSSubClass, this is actually a ordinal variable with a numerical rating. \n",
    "\n",
    "From the description.txt, it seems like it is ordered in the right way. \n",
    "\n",
    "5. **OverallCond**: Rates the overall condition of the house\n",
    "\n",
    "Same feedback as OverallQual\n",
    "\n",
    "6. **YearBuilt**: Original construction date\n",
    "\n",
    "Can be used as-is for now. Keeping in mind, this is a time variable\n",
    "\n",
    "7. **YearRemodAdd**: Remodel date (same as construction date if no remodeling or additions)\n",
    "\n",
    "This is a tricky parameter. we need to figure out what to do with the data points where the house was not remodeled but is copying the construction date\n",
    "\n",
    "Looking at the scatter plot, rest of the data points seem good. My only concern is the piling up of data on 1950\n",
    "\n",
    "Also need to keep in mind this is a time variable\n",
    "\n",
    "8. **MasVnrArea**: Masonry veneer area in square feet\n",
    "Need to remove zero value \n",
    "\n",
    "Asses with  vs without Masonry veneer (consider using as binary variable in the final ML)\n",
    "Then if with Masonry veneer, perform correlation assessment. \n",
    "\n",
    "9. **BsmtFinSF1**: Type 1 finished square feet\n",
    "Need to remove zero values\n",
    "\n",
    "Asses with basement vs without (consider using binary assessment)\n",
    "Then if with basement, need to remove '0' values and perform correlation\n",
    "\n",
    "10. **BsmtFinSF1**: Type 1 finished square feet, **BsmtFinSF2**: Type 2 finished square feet, **BsmtUnfSF**: Unfinished square feet of basement area. **TotalBsmtSF**: Total square feet of basement area\n",
    "\n",
    "Remove zeros\n",
    "Identify creative ways to combine these variables into one, and use some form of a rating to convert into ordinal to perform assessment. Or use decision tree.\n",
    "\n",
    "11. **1stFlrSF**: First Floor square feet, **2ndFlrSF**: Second floor square feet\n",
    "\n",
    "Perform assessment on total square feet.\n",
    "\n",
    "For second floor square feet, remove the zero values\n",
    "\n",
    "12. **LowQualFinSF**: Low quality finished square feet (all floors)\n",
    "\n",
    "This to me should be a binary classifier. There is a vertical line on zero, and all other data points can be represented with almost a horizontal line\n",
    "\n",
    "13. **GrLivArea**: Above grade (ground) living area square feet\n",
    "\n",
    "Destribution of data point is almost identical to 1stFlrSF. This is not a suprise. I don't think we should include this variable. This will diminish the effects of the original variable (1stFlrSF or overall SF)\n",
    "\n",
    "14. **BsmtFullBath**: Basement full bathrooms, **BsmtHalfBath**: Basement half bathrooms, **FullBath**: Full bathrooms above grade, **HalfBath**: Half baths above grade\n",
    "\n",
    "All bathroom formats should be combined into a ordinal format. Need to figure out how. we could do it based on mean. The higher the mean sale price, higher the ordinal value.\n",
    "\n",
    "15. **Bedroom**: Bedrooms above grade (does NOT include basement bedrooms), **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n",
    "\n",
    "Traditionally, this is a huge contributing factor. Lets see how we can capture this\n",
    "\n",
    "Need to combine Bedrooms and total room somehow\n",
    "\n",
    "Decision tree might be a good option for this\n",
    "\n",
    "16. **Kitchen**: Kitchens above grade\n",
    "\n",
    "Very interesting distribution. seems to be a negative correlation\n",
    "\n",
    "17. **Fireplaces**: Number of fireplaces\n",
    "\n",
    "I think we can deprioritize this variable for now, but include in final ML\n",
    "\n",
    "18. **GarageYrBlt**: Year garage was built\n",
    "\n",
    "This is an important variable. Can use as is. But need to remove zeroes\n",
    "\n",
    "19. **GarageCars**: Size of garage in car capacity\n",
    "\n",
    "important variable. looks like good correlation\n",
    "\n",
    "20. **GarageArea**: Size of garage in square feet\n",
    "\n",
    "Good variable. Need to remove zeroes\n",
    "\n",
    "21. **WoodDeckSF**: Wood deck area in square feet, **OpenPorchSF**: Open porch area in square feet, **EnclosedPorch**: Enclosed porch area in square feet, **3SsnPorch**: Three season porch area in square feet, **ScreenPorch**: Screen porch area in square feet, **PoolArea**: Pool area in square feet\n",
    "\n",
    "These can be combined into some \"additional amenities\". can keep the sqft measure. need to decide\n",
    "\n",
    "In any case, need to remove zeroes. \n",
    "\n",
    "22. **MiscVal**: $Value of miscellaneous feature\n",
    "\n",
    "Need to figure out how we can include this. I dont think we can add the value to the total. because value of misc features might actually hold lesser real value.\n",
    "\n",
    "will deprioritize this for now and think about it. \n",
    "\n",
    "23. **MoSold**: Month Sold (MM), **YrSold**: Year Sold (YYYY):\n",
    "\n",
    "Effects of year seems to be limited. Month to Month variability is more significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PILCoMxWPjDM"
   },
   "source": [
    "## The correlations heatmap shows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  1. Below features are highly correlated with SalePrice: \n",
    "\n",
    "- OverallQual\n",
    "- GrLivArea \n",
    "- GarageCars \n",
    "- GarageArea \n",
    "- TotalBsmtSF\n",
    "- 1stFlrSF \n",
    "- FullBath \n",
    "- TotRmsAbvGrd \n",
    "- YearBuilt\n",
    "- YearRemodAdd  \n",
    "\n",
    "###  2. However, there are high correlations also between features themselves\n",
    "\n",
    "-  OverallQual & most other variables\n",
    "-  GrLivArea & TotRMAbvGrd &  FullBath \n",
    "-  GarageCares & GarageArea\n",
    "-  TotalbsmtSF & 1stFlrSF\n",
    "-  BsmtFinSF1 & TotBsmtSF\n",
    "\n",
    "### 3. For first round baseline, we will include below nominal features:\n",
    "\n",
    "-  Yearbuilt \n",
    "-  YearRemodAdd \n",
    "-  GrLivArea \n",
    "-  TotalBsmtSF \n",
    "-  GarageCars\n",
    "-  OverallQual\n",
    "-  LotArea??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn_3uPlHb9FR"
   },
   "source": [
    "### Second pass all the categorical variables\n",
    "\n",
    "we plot the box plots to see whether any category in the variable has a significant different SalePrice. We conclude as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#box plot to explore categorical variables\n",
    "def plot_categorical_boxplots():\n",
    "    #all text variable (categorical)#\n",
    "    df_txt_list=df_EDA.select_dtypes(include={'object'}).columns.to_list()\n",
    "    g = sns.FacetGrid(pd.DataFrame(df_txt_list), col=0, col_wrap=3, aspect=1.5,\n",
    "        sharex=False, sharey=True)\n",
    "    for ax, x_var in zip(g.axes, df_txt_list):\n",
    "        sns.boxplot(data=df, x=x_var, y='SalePrice', ax=ax)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=55)\n",
    "    g.tight_layout()\n",
    "    \n",
    "\n",
    "plot_categorical_boxplots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqGu8jx4PjDQ"
   },
   "source": [
    "### For all the categorical variables, we plot the box plots to see whether any category in the variable has a significant different SalePrice. We conclude as below:\n",
    "\n",
    "####  1. Below features are highly correlated with SalePrice: \n",
    "\n",
    "\n",
    "- The features related to location:\n",
    "\n",
    "    1. Neighborhood: some neighborhoods have significant higher mean price.\n",
    "    2. MSZoning: FV(Floating Village(FV) is a special area where a retirement community was developed and have a higher mean price. Commercial has low mean price. \n",
    "    3. Condition1/Condition2: if there are positive off-site facilities, the prices of properties are higher.\n",
    "\n",
    "- Generally all \"quality\" features, when the quality is excellent or good, the prices is significantly higher.\n",
    "\n",
    "    4. BsmtQual\n",
    "    5. KitchenQual\n",
    "    6. exterCond\n",
    "    7. PoolQC\n",
    "\n",
    "- Features related to the house:\n",
    "\n",
    "    8. MasVnrType: Stone Veneer is higher. \n",
    "    9. GarageType:  BuiltIn is meaningfully higher\n",
    "\n",
    "\n",
    "####  2. However, there are high correlations also between features themselves(will explore more):\n",
    "\n",
    "Such as all quality features, the quality with whether the house has heating and central air\n",
    "\n",
    "#### 3. Some features we will further explore: \n",
    "\n",
    "    1. LandContour\n",
    "    2. BldgType:  TwnhsE/TwnhsI  might have info but not obvious\n",
    "    3. HouseStyle: has info but might be correlated with house sqft\n",
    "    4. RoofMati: has significant difference in values but might also add noises\n",
    "    5. 'ExterQual'/ 'ExterCond': check correlation with other quality\n",
    "    6. BsmtExposure: Gd(good exposure) or No has significant differences in price. check correlation with BsmtQual\n",
    "    7. BsmtFinType1:\n",
    "    8. Exterior1st&2nd\n",
    "    9. functional\n",
    "    10. saletype: new construction is higher but correlated? with yr built\n",
    "    11. SaleCondition'\n",
    "    12. FireplaceQu\n",
    "    13. Heating\n",
    "    14. Central Air\n",
    "    15. HeatingQual\n",
    "    16. garageQual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_hSgKEwPjDR"
   },
   "source": [
    "## From the above assessment it is clear that we might need to heavily depend on learning algorithms that are designed for categorical data as well as nominal data\n",
    "\n",
    "## At this point we will make two lists of \"Important Variables\" from either category, so we can perform an initial baseline assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kboabhnvb9FS"
   },
   "source": [
    "## Below Nominal/Ordinal Variables are prioritized for baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsfFExUbPjDR"
   },
   "source": [
    "\n",
    "1. Yearbuilt\n",
    "We will further process this feature as age of the house (Current Year - Yearbuilt)\n",
    "2. YearRemodAdd\n",
    "We will further process this feature as age of the remodel (Current Year - YearRemodAdd)\n",
    "3. GrLivArea\n",
    "4. Bathroom (Need to sum all the types)\n",
    "5. GarageCars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfqzKU4Jb9FT"
   },
   "source": [
    "## Below Categorical Variables are prioritized for baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFUnpncoPjDR"
   },
   "source": [
    "\n",
    "1. MSSubClass\n",
    "2. MSZoning\n",
    "3. KitchenQual\n",
    "4. Neighborhood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOGGPxc1b9FT"
   },
   "source": [
    "## Selecting a set of feature to work on (THIS IS POST BASIC ANALYSIS STEP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features are selected based on analysis and impact of them on the price\n",
    "\n",
    "['MSSubClass', 'MSZoning', 'LotArea', 'Utilities',\n",
    "                            'Neighborhood', 'OverallQual', 'Age', 'RemodAge',\n",
    "                            'KitchenQual', 'TotalBsmtSF',\n",
    "                            'GarageCars', 'MoSold', 'GrLivArea', 'total_bath']\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGA4X3G6b9FT"
   },
   "source": [
    "## Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWu7HDKMb9FU"
   },
   "source": [
    "## Split data into test and train sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Different Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are training multiple regression models on the Training set and at the end we compare the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Fud_dXVd8P3"
   },
   "source": [
    "## Base Regressor \n",
    "Using this as the base class for all regressor to share the common diagnostics like: Plotting RMSE, and other diagnostics plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QWx58CLHeJGK"
   },
   "outputs": [],
   "source": [
    "class BaseRegressorPlot():  \n",
    "  @staticmethod\n",
    "  def plot_predicted_vs_actual(ax, regressor):\n",
    "    if args.log_level == 'verbose':\n",
    "      print('get_predicted_vs_actual')\n",
    "    predicted, actual = regressor.get_predicted_vs_actual()\n",
    "    ax.scatter(predicted, actual)\n",
    "    ax.set_xlabel(\"Predicted Sales Price ($)\")\n",
    "    ax.set_ylabel(\"Sales Price ($)\")\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_learning_curves(ax, regressor, scaled_data=False):\n",
    "    if scaled_data:\n",
    "        X_train, X_val, y_train, y_val = data_loader.get_scaled_clean_encoded_data()\n",
    "    else:\n",
    "        X_train, X_val, y_train, y_val = data_loader.get_clean_encoded_data()\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        regressor.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = regressor.predict(X_train[:m])\n",
    "        y_val_predict = regressor.predict(X_val)\n",
    "        # calculate RMSE\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict,\n",
    "                                               squared=False))\n",
    "        val_errors.append(mean_squared_error(\n",
    "            y_val, y_val_predict, squared=False))\n",
    "    # ax.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    # ax.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\n",
    "    ax.plot(train_errors, \"r-+\", linewidth=2, label=\"train\")\n",
    "    ax.plot(val_errors, \"b-\", linewidth=3, label=\"val\")\n",
    "    ax.set_xlabel(\"Number of samples\")\n",
    "    ax.set_ylabel(\"RMSE\")\n",
    "\n",
    "  @staticmethod\n",
    "  def plot_history_loss(ax, regressor,  scaled_data=True):\n",
    "    # if scaled_data:\n",
    "    #     X_train, X_val, y_train, y_val = data_loader.get_scaled_clean_encoded_data()\n",
    "    # else:\n",
    "    #     X_train, X_val, y_train, y_val = data_loader.get_clean_encoded_data()\n",
    "    ax.plot(regressor.get_history_loss())\n",
    "  \n",
    "  @staticmethod\n",
    "  def plot_rmse(ax, regressor,  scaled_data=True):\n",
    "    # if scaled_data:\n",
    "    #     X_train, X_val, y_train, y_val = data_loader.get_scaled_clean_encoded_data()\n",
    "    # else:\n",
    "    #     X_train, X_val, y_train, y_val = data_loader.get_clean_encoded_data()\n",
    "    ax.plot(regressor.get_mean_squared_error())\n",
    "\n",
    "  @staticmethod\n",
    "  def clean_out_plot(ax):\n",
    "    ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_linear_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    regressor = LinearRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0, 0], regressor)\n",
    "    BaseRegressorPlot.plot_learning_curves(axes[0, 1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    y_predict_test = regressor.predict(X_test)\n",
    "    print(\"RMSE:\" + str(mean_squared_error(y_test, y_predict_test, squared=False)))\n",
    "\n",
    "if not 'linear' in args.list_of_sections_to_skip:\n",
    "    analyze_linear_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Regressions\n",
    "- Rigde Regression\n",
    "- Lasso Regression\n",
    "- Elstic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8c2R9yQcZCG"
   },
   "source": [
    "## Ridge Regression\n",
    "In this section we analyze Ridge regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is regularized version of linear regression\n",
    "\n",
    "It adds $ \\Sigma_{\\ i=1}^{\\ n} \\ \\theta_i^{\\ 2}$  regularization term to cost function to keep the model weight as samll as possible.\n",
    "\n",
    "Ridge Regression cost function:\n",
    "\n",
    "$$ J(\\theta) \\ = \\ MSE(\\theta) + \\alpha \\ \\frac{1}{2} \\Sigma_{\\ i=1}^{\\ n} \\ \\theta_i^{\\ 2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ridge_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    regressor = RidgeRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor) \n",
    "    BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test),squared=False))\n",
    "\n",
    "if not 'ridge' in args.list_of_sections_to_skip:\n",
    "    analyze_ridge_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso adds a regularization term to the cost function, but it uses the ℓ1 norm of the weight vector instead of half the square of the ℓ2 norm\n",
    "\n",
    "Lasso Regression cost function\n",
    "\n",
    " $$ J(\\theta) \\ = \\ MSE(\\theta) + \\alpha \\  \\Sigma_{\\ i=1}^{\\ n} \\ |\\theta_i| $$\n",
    "\n",
    "An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features (i.e., set them to zero).\n",
    "\n",
    "In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with few nonzero feature weights).\n",
    "\n",
    "Note: Lasso regression has $\\alpha$ as hyperparmeter which needs to be searched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lasso_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    # TODO later use search for alpha\n",
    "    regressor = LassoRegressor(alpha=0.1)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "if not 'lasso' in args.list_of_sections_to_skip:\n",
    "    analyze_lasso_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Elastic Net regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression\n",
    "\n",
    "Elastic Net cost function\n",
    "\n",
    "$$ J(\\theta) \\ = \\ MSE(\\theta) + r \\alpha \\  \\Sigma_{\\ i=1}^{\\ n} \\ |\\theta_i| +  \\frac{1-r}{2} \\alpha \\ \\Sigma_{\\ i=1}^{\\ n} \\ \\theta_i^{\\ 2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_elasticnet_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    # TODO later use search for alpha\n",
    "    regressor = ElasticNetRegressor(alpha=0.1, l1_ratio=0.5)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "if not 'elasticnet' in args.list_of_sections_to_skip:\n",
    "    analyze_elasticnet_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_decision_tree_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'decisontree' in args.list_of_sections_to_skip:\n",
    "    analyze_decision_tree_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_random_forest_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    regressor = RandomForestRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'randomforset' in args.list_of_sections_to_skip:\n",
    "    analyze_random_forest_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for Neural Net we need to use Scaled Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_neuralnet_regression():\n",
    "    X_train, X_test, y_train, y_test = data_loader.get_clean_encoded_data()\n",
    "    regressor = create_nn_regressor(X_train, X_test, y_test, epochs=2)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "    BaseRegressorPlot.plot_predicted_vs_actual(axes[0][0], regressor)\n",
    "    BaseRegressorPlot.plot_history_loss(axes[0][1], regressor)\n",
    "    BaseRegressorPlot.plot_rmse(axes[1][0], regressor)\n",
    "    BaseRegressorPlot.clean_out_plot(axes[1][1])\n",
    "\n",
    "if not 'neuralnet' in args.list_of_sections_to_skip:\n",
    "    analyze_neuralnet_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Filter method\n",
    "-  using correlation to identify important features\n",
    "-  using mutual information (enthropy based)to identify imporatnt features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Corrlation based feature selection \n",
    "\n",
    "\n",
    "(can be potentially better integrated with the EDA corrlation part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fs = data_loader.df\n",
    "X_train_fs, X_test_fs, y_train_fs, y_test_fs = data_loader.get_raw_split_fs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO why did you keep this? Remove after investigation => Jun\n",
    "X_train_fs.columns[X_train_fs.isna().sum(axis=0) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filter_fs = data_loader.data_prep(X_train_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot_correlation(X_train_fs, y_train_fs):\n",
    "    ''' \n",
    "    compute and plot the corr matrix with all features\n",
    " \n",
    "    '''\n",
    "    df_corr = y_train_fs.merge(\n",
    "        X_train_fs, how=\"inner\", left_index=True, right_index=True)\n",
    "    corr_matrix = df_corr.corr()\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix)\n",
    "    plt.title(\"Correlation heatmap\")\n",
    "\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "corr_matrix = get_plot_correlation(X_filter_fs, y_train_fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the features with correlation above cutoff\n",
    "\n",
    "def get_plot_heatmap_top(corr_matrix, thredshod=0.5):\n",
    "    ''' \n",
    "    get above thredshod -- correlation with saleprice above **\n",
    "    plot the correlation heatmap\n",
    "    \n",
    "    '''\n",
    "\n",
    "    top_corr = corr_matrix[corr_matrix[\"SalePrice\"] > thredshod]\n",
    "    top_corr = top_corr.loc[:, top_corr.index]\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(top_corr, annot=True)\n",
    "    plt.title(\"Top heatmap\")\n",
    "\n",
    "    return top_corr\n",
    "\n",
    "\n",
    "top_corr = get_plot_heatmap_top(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Mutual Information based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mutual infomation and plot\n",
    "\n",
    "def make_mi_scores(X, y):\n",
    "\n",
    "    # All discrete features should now have integer dtypes\n",
    "\n",
    "    mi_scores = mutual_info_regression(X, y,  random_state=0)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    scores = scores\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get miscore above 0.1 ( will try 0)\n",
    "\n",
    "mi_scores = make_mi_scores(X_filter_fs, y_train)\n",
    "plot_mi_scores(mi_scores[mi_scores > 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Merge to get the final filtered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_selected=data_loader.get_top_corr_feature(top_corr, mi_scores, X_filter_fs)\n",
    "X_train = data_loader.data_prep(X_train_fs[features_selected])\n",
    "y_train = y_train_fs\n",
    "X_test = data_loader.data_prep(X_test_fs[features_selected])\n",
    "y_test = y_test_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 Fit the new feature set to all the models and calculate the metric(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_linear_regression():\n",
    "\n",
    "    regressor = LinearRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0,0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[0,1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'linear' in args.list_of_sections_to_skip:\n",
    "    analyze_linear_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the observed largest prediction errors (need more work)\n",
    "\n",
    "regressor = LinearRegressor()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_predict_test = regressor.predict(X_test)\n",
    "df_decile = y_test.reset_index().merge(pd.DataFrame(y_predict_test, columns=[\"SalePricePredicted\"]),\n",
    "                                       how=\"inner\", left_index=True, right_index=True).set_index(\"index\")\n",
    "# Calculate diff between predicted and actual for test dataset\n",
    "df_decile['diff'] = (df_decile.SalePrice-df_decile.SalePricePredicted)\n",
    "df_decile['diff_pct'] = (df_decile.SalePricePredicted/df_decile.SalePrice)-1\n",
    "df_decile = df_decile.reset_index().merge(X_train, how=\"inner\", left_index=True,\n",
    "                                          right_index=True).set_index(\"index\")\n",
    "pd.set_option('display.max_rows',\n",
    "              df.shape[0]+1), df_decile.sort_values(by='diff_pct').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ridge_regression():\n",
    "\n",
    "    regressor = RidgeRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'ridge' in args.list_of_sections_to_skip:\n",
    "    analyze_ridge_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lasso_regression():\n",
    "    \n",
    "    # TODO later use search for alpha\n",
    "    regressor = LassoRegressor(alpha=0.1)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "     #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test),squared=False))\n",
    "\n",
    "if not 'lasso' in args.list_of_sections_to_skip:\n",
    "    analyze_lasso_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_elasticnet_regression():\n",
    "    \n",
    "    # TODO later use search for alpha\n",
    "    regressor = ElasticNetRegressor(alpha=0.1, l1_ratio=0.5)\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test),squared=False))\n",
    "    \n",
    "if not 'elasticnet' in args.list_of_sections_to_skip:\n",
    "    analyze_elasticnet_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_decision_tree_regression():\n",
    "    \n",
    "    regressor = DecisionTreeRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test),squared=False))\n",
    "\n",
    "if not 'decisontree' in args.list_of_sections_to_skip:\n",
    "    analyze_decision_tree_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_random_forest_regression():\n",
    "    \n",
    "    regressor = RandomForestRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test),squared=False))\n",
    "\n",
    "if not 'randomforset' in args.list_of_sections_to_skip:\n",
    "    analyze_random_forest_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Wrapper method\n",
    "recursive feature elemination process to identify features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the data to all features\n",
    "df_fs_w = data_loader.df\n",
    "X_train_fs_w, X_test_fs_w, y_train_fs_w, y_test_fs_w = train_test_split(\n",
    "    data_loader.df_X, data_loader.df_y, test_size=0.10, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_loader.data_prep(X_train_fs_w)\n",
    "y_train = y_train_fs_w\n",
    "X_test = data_loader.data_prep(X_test_fs_w)\n",
    "y_test = y_test_fs_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rfe_fs(regressor, threshold=30):\n",
    "    ''' Select the most important * features based on the model\n",
    "         and use it as X_train & X_test'''\n",
    "    regressor = LinearRegression()\n",
    "    selector = RFE(regressor, n_features_to_select=threshold, step=1)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    selector_ind = selector.get_support()\n",
    "    X_train_rfe = X_train.iloc[:, selector_ind]\n",
    "    X_test_rfe = X_test.iloc[:, selector_ind]\n",
    "\n",
    "    return X_train_rfe, X_test_rfe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegressor()\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_linear_regression():\n",
    "\n",
    "    regressor = LinearRegressor()\n",
    "    regressor.fit(X_train_rfe, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 5))\n",
    "\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0,0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[0,1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test_rfe))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test_rfe), squared=False))\n",
    "\n",
    "\n",
    "if not 'linear' in args.list_of_sections_to_skip:\n",
    "    analyze_linear_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RidgeRegressor()\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_ridge_regression():\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'ridge' in args.list_of_sections_to_skip:\n",
    "    analyze_ridge_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LassoRegressor(alpha=0.1)\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_lasso_regression():\n",
    "\n",
    "    # TODO later use search for alpha\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    #plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'lasso' in args.list_of_sections_to_skip:\n",
    "    analyze_lasso_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = ElasticNetRegressor(alpha=0.1, l1_ratio=0.5)\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_elasticnet_regression():\n",
    "\n",
    "    # TODO later use search for alpha\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'elasticnet' in args.list_of_sections_to_skip:\n",
    "    analyze_elasticnet_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = DecisionTreeRegressor()\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_decision_tree_regression():\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'decisontree' in args.list_of_sections_to_skip:\n",
    "    analyze_decision_tree_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = RandomForestRegressor()\n",
    "\n",
    "X_train_rfe, X_test_rfe = rfe_fs(regressor)\n",
    "\n",
    "\n",
    "def analyze_random_forest_regression():\n",
    "\n",
    "    regressor = RandomForestRegressor()\n",
    "    regressor.fit(X_train, y_train)\n",
    "\n",
    "    # plot dignostics\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    #BaseRegressorPlot.plot_predicted_vs_actual(axes[0], regressor)\n",
    "    #BaseRegressorPlot.plot_learning_curves(axes[1], regressor)\n",
    "    plt.scatter(y_test, regressor.predict(X_test))\n",
    "    print(mean_squared_error(y_test, regressor.predict(X_test), squared=False))\n",
    "\n",
    "\n",
    "if not 'randomforset' in args.list_of_sections_to_skip:\n",
    "    analyze_random_forest_regression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1Sfa1Yrb9FV"
   },
   "source": [
    "From Initial Analysis, Random Forest gives best results. We will drill down to find out possible rationale behind it so that it can help us to further calibrate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTwsZ89HPjDU"
   },
   "source": [
    "# Furtherwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2lA9ji5PjDU"
   },
   "source": [
    "The next step, we would like to explore: \n",
    "\n",
    "1. Applying alternative feature selection algorithms see whether it can further improve the accuracy since we observe random forest is giving us the best result.\n",
    "\n",
    "\n",
    "2. Explore whether there are any additional feature engineering we could help to improve the input quality, such as reasonably remove outliers.\n",
    "\n",
    "\n",
    "3. Look into otentially advanced regression techniques\n",
    "\n",
    "\n",
    "4. Apply additional models such as neutral network model to see whether the predictions will be improved."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UqGu8jx4PjDQ"
   ],
   "name": "w207_consolidated.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8f63109110d9088a557ee5ad50e2a3bd89fa997b8cf5cf56bbd0705d6e0c7c35"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
